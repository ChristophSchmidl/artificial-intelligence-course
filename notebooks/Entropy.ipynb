{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "According to Wikipedia (https://en.wikipedia.org/wiki/Entropy_(information_theory)), Entropy in the domain of information theory is defined as follows:\n",
    "\n",
    "*The information entropy, often just entropy, is a basic quantity in information theory associated to any random variable, which can be interpreted as the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper \"A Mathematical Theory of Communication\".*\n",
    "\n",
    "*The entropy is the expected value of the self-information, a related quantity also introduced by Shannon. The self-information quantifies the level of information or surprise associated with one particular outcome or event of a random variable, whereas the entropy quantifies how \"informative\" or \"surprising\" the entire random variable is, averaged on all its possible outcomes.*\n",
    "\n",
    "The basic idea of information theory is that the \"informational value\" of a communicated message depends on the degree to which the content of the message is surprising. If an event is very probable, it is no surprise (and generally uninteresting) when that event happens as expected; hence transmission of such a message carries very little new information. However, if an event is unlikely to occur, it is much more informative to learn that the event happened or will happen. For instance, the knowledge that some particular number will not be the winning number of a lottery provides very little information, because any particular chosen number will almost certainly not win. However, knowledge that a particular number will win a lottery has high value because it communicates the outcome of a very low probability event.\n",
    "\n",
    "The information content (also called the surprisal) of an event $E$ is an increasing function of the reciprocal of the probability $p(E)$ of the event, precisely $I(E)= -\\log _{2}(p(E))=\\log _{2}(1/p(E))$. Entropy measures the expected (i.e., average) amount of information conveyed by identifying the outcome of a random trial. This implies that casting a die has higher entropy than tossing a coin because each outcome of a die toss has smaller probability (about $p=1/6$) than each outcome of a coin toss ($p=1/2$).\n",
    "\n",
    "The entropy (or average entropy) can explicitly be written as:\n",
    "\n",
    "$H(X) = - \\sum_{i = 1}^n P(x_i) \\log_b P(x_i)$\n",
    "\n",
    "where $b$ is the base of the logarithm used. Common values of b are 2, Euler's number $e$, and 10, and the corresponding units of entropy are the bits for $b = 2$, nats for $b = e$, and bans for $b = 10$.\n",
    "\n",
    "Taken from Paul Wilmott:\n",
    "\n",
    "Surprisal or information content associated with an event is the negative of the logarithm of the probability of the event \n",
    "\n",
    "$- \\log_2(p)$ \n",
    "\n",
    "Consider a coin toss of four coins like HTHH. This could be represented by 0100. How many states are possible by this notation? -> 2 * 2 * 2 * 2 = 16. How many bits do I need to represent 16 states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def surprisal(x, base = \"e\"):\n",
    "    ''' Also called information content'''\n",
    "    if(base == 2):\n",
    "        return -np.log2(x)\n",
    "    if(base == 10): \n",
    "        return -np.log10(x)\n",
    "    if(base == \"e\"): \n",
    "        return -np.log(x)    \n",
    "\n",
    "def entropy(arr, base = \"e\"):\n",
    "    '''returns sum_{i = 1}^n - log_b(x_i) * x_i  which is the same as - sum_{i = 1}^n x_i * log_b(x_i) '''\n",
    "    return sum((lambda x : surprisal(x, base) * x)(x) for x in arr )\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0) \n",
    "\n",
    "def normalize_to_probs(arr):\n",
    "    summed = sum(arr)\n",
    "    return [ (lambda x : x / summed)(x) for x in arr ]\n",
    "    \n",
    "    \n",
    "print(np.log2(16))\n",
    "\n",
    "# is the same as\n",
    "print(surprisal(1/16, 2))\n",
    "# Given that the probability of the event is 1/16, the \"surprisal\" would be 4 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax performs the following transform on n numbers $x_1,... x_n$:\n",
    "\n",
    "$s(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_kj}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that I now have a biased coin where the probability of heads is 3/4 and tails 1/4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4150374992788438\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# heads\n",
    "print(surprisal(3/4, 2))\n",
    "# tails\n",
    "print(surprisal(1/4, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we put the probabilites of the biased coin into an array and want to calculate the (average) entropy of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "probs_biased = [3/4, 1/4]\n",
    "\n",
    "print(entropy(probs_biased, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a fair coint it would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "probs_fair = [0.5, 0.5]\n",
    "\n",
    "print(entropy(probs_fair, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we do not have the probabilities but only the discrete occurences of each event in an array. To transform these values to probabilites which later on sum to 1 we could simply scale each entry (entry / sum(array)) and feed the result to our entropy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.3, 0.4, 0.1]\n",
      "[0.08714432 0.23688282 0.64391426 0.0320586 ]\n",
      "1.0\n",
      "1.0\n",
      "1.8464393446710154\n"
     ]
    }
   ],
   "source": [
    "counts = [2, 3, 4, 1]\n",
    "normalized = normalize_to_probs(counts)\n",
    "# https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\n",
    "# https://victorzhou.com/blog/softmax/  <---- very nice\n",
    "softmaxed = softmax(counts) \n",
    "# Softmax is not behaving as I expected. Because it uses the exponential function for scaling\n",
    "# and also works with negative values?\n",
    "print(normalized)\n",
    "print(softmaxed)\n",
    "\n",
    "print(sum(normalized))\n",
    "print(sum(softmaxed))\n",
    "\n",
    "print(entropy(normalized, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
